{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPaUwvS0nn//Wwx78aCqc7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seyviour/enel645-g12/blob/main/645_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach**\n",
        "\n",
        "We finetuned Paligemma, a 3-Billion parameter vision-language model developed by Google, using LoRA to make the training process more efficient. We framed the task as a multiple-choice-question answering problem, and achieved an accuracy of ~86% on the test set.\n",
        "\n",
        "Final training was done on the ARC cluster with a batch size of 8 and a 1e-5 learning rate which we found, experimentally, to give the best results. Inference was done on Ziheng's 4070.\n",
        "\n"
      ],
      "metadata": {
        "id": "z0nuNz1yxyIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "\n",
        "#Utils\n",
        "import os\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "#Modelling and computation\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer\n",
        "from transformers import PaliGemmaForConditionalGeneration\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from transformers import PaliGemmaProcessor\n",
        "#Huggingface\n",
        "from huggingface_hub import login\n",
        "#Dataset and Transforms\n",
        "from torch.utils.data import Dataset, ConcatDataset\n",
        "from torchvision import transforms\n",
        "from transformers import TrainingArguments\n",
        "#Evaluation\n",
        "# import evaluate\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "#Quantization\n",
        "from transformers import BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "rHFrB2lpznVy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Folders\n",
        "train_folder = \"/home/ziheng.chang/garbage_data/garbage_data/CVPR_2024_dataset_Train\"\n",
        "test_folder = \"/home/ziheng.chang/garbage_data/garbage_data/CVPR_2024_dataset_Test\"\n",
        "val_folder = \"/home/ziheng.chang/garbage_data/garbage_data/CVPR_2024_dataset_Val\""
      ],
      "metadata": {
        "id": "e9VUkZaN2uaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup\n",
        "RANDOM_SEED = 42\n",
        "UAT = \"\" #Set hugging face access token here\n",
        "\n",
        "HUGGINGFACE_USER_NAME = \"palicoqiqi\"\n",
        "HUGGINGFACE_USER_NAME = \"seyviour\"\n",
        "\n",
        "IS_TEST_JOB = False\n",
        "# IS_TEST_JOB = True\n",
        "\n",
        "UAT = UAT or os.getenv(\"HF_TOKEN\")\n",
        "if UAT is None:\n",
        "    raise Exception(\"Hugging Face Token is not set\")\n",
        "os.environ[\"HF_TOKEN\"] = UAT #hugging face token\n",
        "login(token=UAT)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_id = \"google/paligemma-3b-pt-224\""
      ],
      "metadata": {
        "id": "TBClYLe421pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yl2lg1da7_mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "      A PyTorch Dataset for loading and transforming image-text data for a multiple-choice\n",
        "      question-answering task, designed to categorize images based on visual and textual cues.\n",
        "\n",
        "      Attributes:\n",
        "      ----------\n",
        "      base_folder : str\n",
        "          The root directory containing the dataset folders.\n",
        "      sub_folder : str\n",
        "          The specific subdirectory within `base_folder` corresponding to a class of images.\n",
        "      label : str\n",
        "          The label associated with this dataset, typically the category of the images.\n",
        "      base_transform : callable, optional\n",
        "          The base transformation applied to each image (e.g., normalization).\n",
        "      aug_transforms : list of callables, optional\n",
        "          A list of augmentation transformations applied to each image to increase dataset diversity.\n",
        "      encoder : callable, optional\n",
        "          An optional encoder for the text descriptions, if required.\n",
        "      max_size : int, optional\n",
        "          Maximum number of images to load from `sub_folder`. If None, all images are loaded.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_folder, sub_folder, label, base_transform=None,\n",
        "                 aug_transforms=None, encoder=None, max_size=None\n",
        "                 ):\n",
        "\n",
        "        self.aug_transforms = [] if aug_transforms is None else aug_transforms\n",
        "        self.base_folder = base_folder\n",
        "        self.sub_folder = sub_folder\n",
        "        self.base_transform = base_transform\n",
        "        self._path = os.path.join(self.base_folder, self.sub_folder)\n",
        "        self._file_names = sorted(os.listdir(self._path))\n",
        "        self.max_size = max_size\n",
        "        if (self.max_size):\n",
        "            self._file_names = self._file_names[:self.max_size]\n",
        "        self.encoder = encoder\n",
        "        self.label = label\n",
        "\n",
        "    def _get_image_path(self, idx):\n",
        "        full_path = os.path.join(self._path, self._file_names[idx])\n",
        "        return full_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._file_names) * (len(self.aug_transforms) + 1)\n",
        "\n",
        "    def to_description(self, file_name):\n",
        "        file_name_no_ext, _ = os.path.splitext(file_name)\n",
        "        text = file_name_no_ext.replace('_', ' ')\n",
        "        text_without_digits = re.sub(r'\\d+', '', text)\n",
        "        return text_without_digits.strip()\n",
        "\n",
        "    def _get_image_at_idx(self, idx):\n",
        "        transform_idx = idx//(len(self._file_names)+1)\n",
        "        true_idx = idx % (len(self._file_names))\n",
        "        filepath = self._get_image_path(true_idx)\n",
        "        image = Image.open(filepath).convert('RGB')  # Convert to RGB\n",
        "        if transform_idx:\n",
        "            # Augment the Dataset by applying a transform when `idx` is greater\n",
        "            # than the number of files in the subfolder\n",
        "            image = self.aug_transforms[transform_idx-1](image)\n",
        "        if self.base_transform:\n",
        "            image = self.base_transform(image)\n",
        "        return image\n",
        "\n",
        "    def _get_text_at_idx(self, idx):\n",
        "        true_idx = idx % (len(self._file_names))\n",
        "        filename = self._file_names[true_idx]\n",
        "        text = self.to_description(filename)\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns a dictionary containing the image, question, and label for the specified index.\n",
        "        description = self._get_text_at_idx(idx)\n",
        "        image = self._get_image_at_idx(idx)\n",
        "        label = self.label\n",
        "        data_point = {\n",
        "            'multiple_choice_answer': label,\n",
        "            'question': f'What type of garbage is this {description}?',\n",
        "            'image': image\n",
        "        }\n",
        "        return data_point\n"
      ],
      "metadata": {
        "id": "oi8fg1zf0kTD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper for creating datasets\n",
        "def make_garbage_dataset(basefolder:str, key, kwargs)->tuple[ConcatDataset, dict]:\n",
        "    # Ensure folders are sorted, exclude hidden files/folders\n",
        "    class_folders = sorted([x for x in os.listdir(basefolder) if x[0]!='.'])\n",
        "    individual_datasets = [ImageTextDataset(basefolder, x, key.get(x.lower(), x), **kwargs) for x in class_folders]\n",
        "    return ConcatDataset(individual_datasets)"
      ],
      "metadata": {
        "id": "S0n4nWc33SWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_names = ['Black', 'Green', 'Blue', 'TTR']\n",
        "garbage_types = ['black', 'green', 'blue', 'other'] #Corresponds to ['landfill', 'compostable', 'recyclable', 'other']\n",
        "garbage_types = [\"landfill\", \"compostable\", \"recyclable\", \"other\"]\n",
        "\n",
        "garbage_types = {\n",
        "      \"black\" : \"landfill\",\n",
        "      \"green\": \"compostable\",\n",
        "      \"blue\": \"recyclable\",\n",
        "      \"ttr\": \"other\"\n",
        "    }"
      ],
      "metadata": {
        "id": "fK3Cv9rT2mPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Transformations. Paligemma's `process` function also applies transforms\n",
        "#that prepare the images for processing by PaliGemma, so we apply only basic\n",
        "#transforms here\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(0.5)\n",
        "        ])\n",
        "\n",
        "aug_transform = transforms.v2.RandomChoice([\n",
        "    transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=.5, hue=.3),\n",
        "        transforms.RandomRotation(degrees = (0,45))\n",
        "        ]\n",
        "    ),\n",
        "    transforms.Compose([\n",
        "        transforms.GaussianBlur(kernel_size = (5,5)),\n",
        "        transforms.RandomRotation(degrees = (0,45))\n",
        "        ]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "L42_GrnP3fxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100 if IS_TEST_JOB else None\n",
        "kwargs_train = {\n",
        "    \"base_transform\": preprocess,\n",
        "    \"aug_transforms\": [aug_transform],\n",
        "    \"max_len\": max_len\n",
        "}\n",
        "\n",
        "kwargs_eval = {\n",
        "    \"base_transform\": preprocess,\n",
        "    \"aug_transforms\": None,\n",
        "    \"max_len\": max_len\n",
        "}\n",
        "\n",
        "train_data = make_garbage_dataset(train_folder, garbage_types, kwargs_train)\n",
        "test_data = make_garbage_dataset(test_folder, garbage_types, kwargs_eval)\n",
        "val_data = make_garbage_dataset(test_folder, garbage_types, kwargs_eval)"
      ],
      "metadata": {
        "id": "g7yiapcv3zEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Parameters: LoRA & QLoRA fine-tuning to reduce cost of training\n",
        "!pip install bitsandbytes\n",
        "model_id = \"google/paligemma-3b-pt-224\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    #Paligemma modules to apply LoRA to\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "zFen2ykG38nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
        "# Collate function passed to the Trainer for creating batches of training data\n",
        "def collate_fn(examples):\n",
        "  texts = [\"<image> <bos> answer \" + example[\"question\"] for example in examples]\n",
        "  labels= [example['multiple_choice_answer'] for example in examples]\n",
        "  images = [example[\"image\"].convert(\"RGB\") for example in examples]\n",
        "  tokens = processor(text=texts, images=images, suffix=labels,\n",
        "                    return_tensors=\"pt\", padding=\"longest\")\n",
        "  tokens = tokens.to(torch.bfloat16).to(device)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "HIPaRInR6Kmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get base model from Huggingfacehub\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0}\n",
        "  )\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n"
      ],
      "metadata": {
        "id": "KFZ5pw1u6iZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # Setup training, then train\n",
        "#Initialize the `TrainingArguments`.\n",
        "# Reasonable parameters are chosen after experimentation.\n",
        "# 12 epochs gives good results without running for too long.\n",
        "# 8 batch size gives good speed withou OOM\n",
        "# learning rate of 4e-5 gives best accuracy\n",
        "# We experimented with multiple learning rates and found that 4e-5 gave\n",
        "# The best accuracy\n",
        "TRAIN_BATCH_SIZE = 3\n",
        "EVAL_BATCH_SIZE = 3\n",
        "\n",
        "args=TrainingArguments(\n",
        "            num_train_epochs=12,\n",
        "            remove_unused_columns=False,\n",
        "            per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "            per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=2,\n",
        "            learning_rate=0.00004,\n",
        "            weight_decay=1e-6,\n",
        "            adam_beta2=0.999,\n",
        "            optim=\"adamw_torch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            push_to_hub=True,\n",
        "            save_total_limit=1,\n",
        "            output_dir=\"paligemma_vqav2_2\",\n",
        "            bf16=True,\n",
        "            report_to=[\"tensorboard\"],\n",
        "            dataloader_pin_memory=False,\n",
        "            load_best_model_at_end=True,\n",
        "            eval_do_concat_batches = False\n",
        "        )\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_data,\n",
        "        data_collator=collate_fn,\n",
        "        args=args\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwktX3T243v8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save the model for inference/testing\n",
        "trainer.train()\n",
        "\n",
        "#Push the model to huggingface hub for future use.\n",
        "#Also save it locally just in case\n",
        "trainer.push_to_hub(f'{HUGGINGFACE_USER_NAME}/paligemma_VQAv2_enel645_2')\n",
        "trainer.save_model(\"model/paligemma_model_2\")"
      ],
      "metadata": {
        "id": "4XteQBxy5VZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(examples):\n",
        "    predicted = []\n",
        "    actual = []\n",
        "    count = 0\n",
        "    tot_count = len(examples)\n",
        "    for example in examples:\n",
        "        texts = \"<image> <bos> answer \" + example[\"question\"]\n",
        "        labels= example['multiple_choice_answer']\n",
        "        images = example[\"image\"].convert(\"RGB\")\n",
        "\n",
        "        # Preprocessing Inputs\n",
        "        inputs = processor(text=texts, images=images, padding=\"longest\", do_convert_rgb=True, return_tensors=\"pt\").to(device)\n",
        "        inputs = inputs.to(dtype=model.dtype)\n",
        "\n",
        "        # Generating and Decoding Output\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs, max_length=496)\n",
        "\n",
        "        output = processor.decode(output[0], skip_special_tokens=True)\n",
        "        output = output.lower()\n",
        "        options = sorted([\"black\", \"blue\", \"ttr\", \"green\"])\n",
        "        if output not in options:\n",
        "            print('Wrong prediction detected:',processor.decode(output[0], skip_special_tokens=True), \"\\n\")\n",
        "            predicted.append(len(options))\n",
        "        else:\n",
        "            predicted.append(options.index(output))\n",
        "        if count % 500 == 0:\n",
        "            Accuracy = metrics.accuracy_score(actual, predicted)\n",
        "            print(count/tot_count*100, '% done. Accuracy so far is', Accuracy*100, '%.')\n",
        "        count += 1\n",
        "\n",
        "    print('Completed.')\n",
        "    return actual, predicted\n",
        "\n",
        "actual, predicted = make_predictions(test_data)"
      ],
      "metadata": {
        "id": "C3ep1l5o5Rr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NiwRTR-JVYMF"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "7KShur6ywjMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dUU6k3q_zk0c"
      }
    }
  ]
}